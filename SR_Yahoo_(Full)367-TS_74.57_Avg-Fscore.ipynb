{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ம ஹா ம்ரி த் யு ஞ் ஜ யா\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nம ஹா ம்ரி த் யு ஞ் ஜ யா\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "from pathlib import Path\n",
    "#from tensorflow.contrib.rnn import LSTMCell, GRUCell\n",
    "import numpy as np\n",
    "import time, uuid\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Box_filter(values, kernel_size=3):\n",
    "    \"\"\"\n",
    "    :return: The list of filtered average\n",
    "    \"\"\"\n",
    "    filter_values = np.cumsum(values, dtype=float)\n",
    "\n",
    "    filter_values[kernel_size:] = filter_values[kernel_size:] - filter_values[:-kernel_size]\n",
    "    filter_values[kernel_size:] = filter_values[kernel_size:] / kernel_size\n",
    "\n",
    "    for i in range(1, kernel_size):\n",
    "        filter_values[i] /= i + 1\n",
    "\n",
    "    return filter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_next(values):\n",
    "    \"\"\"\n",
    "    Extrapolates the next value by sum up the slope of the last value with previous values.\n",
    "    :param values: a list or numpy array of time-series\n",
    "    :return: the next value of time-series\n",
    "    \"\"\"\n",
    "\n",
    "    last_value = values[-1]\n",
    "    slope = [(last_value - v) / i for (i, v) in enumerate(values[::-1])]\n",
    "    slope[0] = 0\n",
    "    next_values = last_value + np.cumsum(slope)\n",
    "\n",
    "    return next_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_series(values, extend_num=5, forward=5):\n",
    "\n",
    "    next_value = extrapolate_next(values)[forward]\n",
    "    extension = [next_value] * extend_num\n",
    "\n",
    "    if isinstance(values, list):\n",
    "        merge_values = values + extension\n",
    "    else:\n",
    "        merge_values = np.append(values, extension)\n",
    "    return merge_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Silency(object):\n",
    "    def __init__(self, amp_window_size, series_window_size, score_window_size):\n",
    "        self.amp_window_size = amp_window_size\n",
    "        self.series_window_size = series_window_size\n",
    "        self.score_window_size = score_window_size\n",
    "\n",
    "    def frequency_domain_map(self, values):\n",
    "        freq = np.fft.fft(values)\n",
    "        mag = np.sqrt(freq.real ** 2 + freq.imag ** 2)\n",
    "        spectral_residual = np.exp(np.log(mag) - Box_filter(np.log(mag), self.amp_window_size))\n",
    "\n",
    "        freq.real = freq.real * spectral_residual / mag\n",
    "        freq.imag = freq.imag * spectral_residual / mag\n",
    "\n",
    "        freqmap = np.fft.ifft(freq)\n",
    "        freq_map = np.sqrt(freqmap.real ** 2 + freqmap.imag ** 2)\n",
    "        return freq_map\n",
    "\n",
    "    def generate_anomaly_score(self, values, type=\"avg\"):\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "\n",
    "        ave_filter = Box_filter(mag, self.score_window_size)        \n",
    "        score = (mag - ave_filter) / ave_filter \n",
    "        \n",
    "        return score\n",
    "\n",
    "    def generate_anomaly_score2(self, values, type=\"avg\"):\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "\n",
    "        return mag\n",
    "\n",
    "    def generate_anomaly_score_(self, values, type=\"avg\"):\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "\n",
    "        ave_filter = Box_filter(mag, self.score_window_size)\n",
    "        \n",
    "        ave_filter1 = Box_filter(ave_filter, 12)\n",
    "        ave_filter2 = Box_filter(ave_filter1, 8)\n",
    "        ave_filter = Box_filter(ave_filter2, 6)\n",
    "        \n",
    "        score = (mag - ave_filter) / ave_filter\n",
    "        \n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire A4 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/manjunath.adinarayan/TIME-SERIES/YAHOO_Data/Full_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "extension='csv'\n",
    "os.chdir( path )\n",
    "files = glob.glob('*.{}'.format(extension))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real_59.csv',\n",
       " 'real_65.csv',\n",
       " 'A3Benchmark-TS12.csv',\n",
       " 'synthetic_85.csv',\n",
       " 'synthetic_91.csv',\n",
       " 'A4Benchmark-TS99.csv',\n",
       " 'A4Benchmark-TS72.csv',\n",
       " 'synthetic_46.csv',\n",
       " 'synthetic_52.csv',\n",
       " 'A4Benchmark-TS66.csv',\n",
       " 'synthetic_53.csv',\n",
       " 'A4Benchmark-TS67.csv',\n",
       " 'A4Benchmark-TS73.csv',\n",
       " 'synthetic_47.csv',\n",
       " 'A4Benchmark-TS98.csv',\n",
       " 'synthetic_90.csv',\n",
       " 'synthetic_84.csv',\n",
       " 'A3Benchmark-TS13.csv',\n",
       " 'real_64.csv',\n",
       " 'real_58.csv',\n",
       " 'real_66.csv',\n",
       " 'A3Benchmark-TS11.csv',\n",
       " 'A3Benchmark-TS39.csv',\n",
       " 'synthetic_92.csv',\n",
       " 'synthetic_86.csv',\n",
       " 'A4Benchmark-TS65.csv',\n",
       " 'synthetic_51.csv',\n",
       " 'synthetic_45.csv',\n",
       " 'A4Benchmark-TS71.csv',\n",
       " 'A4Benchmark-TS59.csv',\n",
       " 'synthetic_79.csv',\n",
       " 'synthetic_78.csv',\n",
       " 'synthetic_100.csv',\n",
       " 'A4Benchmark-TS58.csv',\n",
       " 'synthetic_44.csv',\n",
       " 'A4Benchmark-TS70.csv',\n",
       " 'A4Benchmark-TS64.csv',\n",
       " 'synthetic_50.csv',\n",
       " 'synthetic_87.csv',\n",
       " 'synthetic_93.csv',\n",
       " 'A3Benchmark-TS38.csv',\n",
       " 'A3Benchmark-TS10.csv',\n",
       " 'real_67.csv',\n",
       " 'real_63.csv',\n",
       " 'A3Benchmark-TS28.csv',\n",
       " 'A3Benchmark-TS14.csv',\n",
       " 'synthetic_97.csv',\n",
       " 'synthetic_83.csv',\n",
       " 'synthetic_68.csv',\n",
       " 'A4Benchmark-TS48.csv',\n",
       " 'synthetic_54.csv',\n",
       " 'A4Benchmark-TS60.csv',\n",
       " 'A4Benchmark-TS74.csv',\n",
       " 'synthetic_40.csv',\n",
       " 'A4Benchmark-TS75.csv',\n",
       " 'synthetic_41.csv',\n",
       " 'synthetic_55.csv',\n",
       " 'A4Benchmark-TS61.csv',\n",
       " 'A4Benchmark-TS49.csv',\n",
       " 'synthetic_69.csv',\n",
       " 'synthetic_82.csv',\n",
       " 'synthetic_96.csv',\n",
       " 'A3Benchmark-TS15.csv',\n",
       " 'A3Benchmark-TS29.csv',\n",
       " 'real_62.csv',\n",
       " 'real_60.csv',\n",
       " 'real_48.csv',\n",
       " 'A3Benchmark-TS17.csv',\n",
       " 'A4Benchmark-TS88.csv',\n",
       " 'synthetic_80.csv',\n",
       " 'synthetic_94.csv',\n",
       " 'synthetic_43.csv',\n",
       " 'A4Benchmark-TS77.csv',\n",
       " 'A4Benchmark-TS63.csv',\n",
       " 'synthetic_57.csv',\n",
       " 'A4Benchmark-TS62.csv',\n",
       " 'synthetic_56.csv',\n",
       " 'synthetic_42.csv',\n",
       " 'A4Benchmark-TS76.csv',\n",
       " 'synthetic_95.csv',\n",
       " 'synthetic_81.csv',\n",
       " 'A4Benchmark-TS89.csv',\n",
       " 'A3Benchmark-TS16.csv',\n",
       " 'real_49.csv',\n",
       " 'real_61.csv',\n",
       " 'real_12.csv',\n",
       " 'real_5.csv',\n",
       " 'A3Benchmark-TS65.csv',\n",
       " 'A3Benchmark-TS71.csv',\n",
       " 'A3Benchmark-TS59.csv',\n",
       " 'A3Benchmark-TS5.csv',\n",
       " 'synthetic_25.csv',\n",
       " 'A4Benchmark-TS11.csv',\n",
       " 'synthetic_31.csv',\n",
       " 'synthetic_19.csv',\n",
       " 'A4Benchmark-TS39.csv',\n",
       " 'A4Benchmark-TS38.csv',\n",
       " 'synthetic_18.csv',\n",
       " 'synthetic_30.csv',\n",
       " 'synthetic_24.csv',\n",
       " 'A4Benchmark-TS10.csv',\n",
       " 'A3Benchmark-TS4.csv',\n",
       " 'A3Benchmark-TS58.csv',\n",
       " 'A3Benchmark-TS70.csv',\n",
       " 'A3Benchmark-TS64.csv',\n",
       " 'real_4.csv',\n",
       " 'real_13.csv',\n",
       " 'real_39.csv',\n",
       " 'real_11.csv',\n",
       " 'A4Benchmark-TS8.csv',\n",
       " 'real_6.csv',\n",
       " 'A3Benchmark-TS72.csv',\n",
       " 'A3Benchmark-TS66.csv',\n",
       " 'A3Benchmark-TS6.csv',\n",
       " 'A3Benchmark-TS99.csv',\n",
       " 'synthetic_32.csv',\n",
       " 'A4Benchmark-TS12.csv',\n",
       " 'synthetic_26.csv',\n",
       " 'A4Benchmark-TS13.csv',\n",
       " 'synthetic_27.csv',\n",
       " 'synthetic_33.csv',\n",
       " 'A3Benchmark-TS98.csv',\n",
       " 'A3Benchmark-TS7.csv',\n",
       " 'A3Benchmark-TS67.csv',\n",
       " 'A3Benchmark-TS73.csv',\n",
       " 'real_7.csv',\n",
       " 'A4Benchmark-TS9.csv',\n",
       " 'A4Benchmark-TS100.csv',\n",
       " 'real_10.csv',\n",
       " 'real_38.csv',\n",
       " 'real_14.csv',\n",
       " 'real_28.csv',\n",
       " 'real_3.csv',\n",
       " 'synthetic_8.csv',\n",
       " 'A3Benchmark-TS77.csv',\n",
       " 'A3Benchmark-TS63.csv',\n",
       " 'A3Benchmark-TS88.csv',\n",
       " 'A3Benchmark-TS3.csv',\n",
       " 'synthetic_37.csv',\n",
       " 'synthetic_23.csv',\n",
       " 'A4Benchmark-TS17.csv',\n",
       " 'synthetic_22.csv',\n",
       " 'A4Benchmark-TS16.csv',\n",
       " 'synthetic_36.csv',\n",
       " 'A3Benchmark-TS2.csv',\n",
       " 'A3Benchmark-TS89.csv',\n",
       " 'A3Benchmark-TS62.csv',\n",
       " 'A3Benchmark-TS76.csv',\n",
       " 'synthetic_9.csv',\n",
       " 'real_2.csv',\n",
       " 'real_29.csv',\n",
       " 'real_15.csv',\n",
       " 'real_17.csv',\n",
       " 'A3Benchmark-TS48.csv',\n",
       " 'A3Benchmark-TS60.csv',\n",
       " 'A3Benchmark-TS74.csv',\n",
       " 'A4Benchmark-TS28.csv',\n",
       " 'A4Benchmark-TS14.csv',\n",
       " 'synthetic_20.csv',\n",
       " 'synthetic_34.csv',\n",
       " 'synthetic_35.csv',\n",
       " 'A4Benchmark-TS15.csv',\n",
       " 'synthetic_21.csv',\n",
       " 'A4Benchmark-TS29.csv',\n",
       " 'A3Benchmark-TS1.csv',\n",
       " 'A3Benchmark-TS75.csv',\n",
       " 'A3Benchmark-TS61.csv',\n",
       " 'A3Benchmark-TS49.csv',\n",
       " 'real_1.csv',\n",
       " 'real_16.csv',\n",
       " 'real_33.csv',\n",
       " 'real_27.csv',\n",
       " 'A4Benchmark-TS2.csv',\n",
       " 'synthetic_7.csv',\n",
       " 'A3Benchmark-TS44.csv',\n",
       " 'A3Benchmark-TS50.csv',\n",
       " 'A3Benchmark-TS78.csv',\n",
       " 'A3Benchmark-TS87.csv',\n",
       " 'A3Benchmark-TS93.csv',\n",
       " 'A4Benchmark-TS30.csv',\n",
       " 'A4Benchmark-TS24.csv',\n",
       " 'synthetic_10.csv',\n",
       " 'synthetic_38.csv',\n",
       " 'A4Benchmark-TS18.csv',\n",
       " 'A4Benchmark-TS19.csv',\n",
       " 'synthetic_39.csv',\n",
       " 'A4Benchmark-TS25.csv',\n",
       " 'synthetic_11.csv',\n",
       " 'A4Benchmark-TS31.csv',\n",
       " 'A3Benchmark-TS92.csv',\n",
       " 'A3Benchmark-TS86.csv',\n",
       " 'A3Benchmark-TS79.csv',\n",
       " 'A3Benchmark-TS51.csv',\n",
       " 'A3Benchmark-TS45.csv',\n",
       " 'synthetic_6.csv',\n",
       " 'A4Benchmark-TS3.csv',\n",
       " 'real_26.csv',\n",
       " 'real_32.csv',\n",
       " 'real_18.csv',\n",
       " 'real_24.csv',\n",
       " 'real_30.csv',\n",
       " 'A4Benchmark-TS1.csv',\n",
       " 'synthetic_4.csv',\n",
       " 'A3Benchmark-TS53.csv',\n",
       " 'A3Benchmark-TS47.csv',\n",
       " 'A3Benchmark-TS90.csv',\n",
       " 'A3Benchmark-TS84.csv',\n",
       " 'synthetic_13.csv',\n",
       " 'A4Benchmark-TS27.csv',\n",
       " 'A4Benchmark-TS33.csv',\n",
       " 'A4Benchmark-TS32.csv',\n",
       " 'synthetic_12.csv',\n",
       " 'A4Benchmark-TS26.csv',\n",
       " 'A3Benchmark-TS85.csv',\n",
       " 'A3Benchmark-TS91.csv',\n",
       " 'A3Benchmark-TS46.csv',\n",
       " 'A3Benchmark-TS52.csv',\n",
       " 'synthetic_5.csv',\n",
       " 'real_31.csv',\n",
       " 'real_25.csv',\n",
       " 'real_19.csv',\n",
       " 'real_21.csv',\n",
       " 'real_35.csv',\n",
       " 'A4Benchmark-TS4.csv',\n",
       " 'synthetic_1.csv',\n",
       " 'A3Benchmark-TS56.csv',\n",
       " 'A3Benchmark-TS42.csv',\n",
       " 'A3Benchmark-TS95.csv',\n",
       " 'A3Benchmark-TS81.csv',\n",
       " 'A4Benchmark-TS22.csv',\n",
       " 'synthetic_16.csv',\n",
       " 'A4Benchmark-TS36.csv',\n",
       " 'A4Benchmark-TS37.csv',\n",
       " 'A4Benchmark-TS23.csv',\n",
       " 'synthetic_17.csv',\n",
       " 'A3Benchmark-TS80.csv',\n",
       " 'A3Benchmark-TS94.csv',\n",
       " 'A3Benchmark-TS43.csv',\n",
       " 'A3Benchmark-TS57.csv',\n",
       " 'A4Benchmark-TS5.csv',\n",
       " 'real_34.csv',\n",
       " 'real_20.csv',\n",
       " 'real_36.csv',\n",
       " 'real_22.csv',\n",
       " 'A4Benchmark-TS7.csv',\n",
       " 'real_9.csv',\n",
       " 'synthetic_2.csv',\n",
       " 'A3Benchmark-TS69.csv',\n",
       " 'A3Benchmark-TS41.csv',\n",
       " 'A3Benchmark-TS55.csv',\n",
       " 'A3Benchmark-TS9.csv',\n",
       " 'A3Benchmark-TS82.csv',\n",
       " 'A3Benchmark-TS96.csv',\n",
       " 'synthetic_29.csv',\n",
       " 'A4Benchmark-TS35.csv',\n",
       " 'synthetic_15.csv',\n",
       " 'A4Benchmark-TS21.csv',\n",
       " 'synthetic_14.csv',\n",
       " 'A4Benchmark-TS20.csv',\n",
       " 'A4Benchmark-TS34.csv',\n",
       " 'synthetic_28.csv',\n",
       " 'A3Benchmark-TS97.csv',\n",
       " 'A3Benchmark-TS83.csv',\n",
       " 'A3Benchmark-TS8.csv',\n",
       " 'A3Benchmark-TS54.csv',\n",
       " 'A3Benchmark-TS40.csv',\n",
       " 'A3Benchmark-TS68.csv',\n",
       " 'synthetic_3.csv',\n",
       " 'real_8.csv',\n",
       " 'A4Benchmark-TS6.csv',\n",
       " 'real_23.csv',\n",
       " 'real_37.csv',\n",
       " 'real_50.csv',\n",
       " 'real_44.csv',\n",
       " 'A3Benchmark-TS100.csv',\n",
       " 'A3Benchmark-TS27.csv',\n",
       " 'A3Benchmark-TS33.csv',\n",
       " 'A4Benchmark-TS90.csv',\n",
       " 'A4Benchmark-TS84.csv',\n",
       " 'synthetic_98.csv',\n",
       " 'A4Benchmark-TS53.csv',\n",
       " 'synthetic_67.csv',\n",
       " 'synthetic_73.csv',\n",
       " 'A4Benchmark-TS47.csv',\n",
       " 'synthetic_72.csv',\n",
       " 'A4Benchmark-TS46.csv',\n",
       " 'A4Benchmark-TS52.csv',\n",
       " 'synthetic_66.csv',\n",
       " 'synthetic_99.csv',\n",
       " 'A4Benchmark-TS85.csv',\n",
       " 'A4Benchmark-TS91.csv',\n",
       " 'A3Benchmark-TS32.csv',\n",
       " 'A3Benchmark-TS26.csv',\n",
       " 'real_45.csv',\n",
       " 'real_51.csv',\n",
       " 'real_47.csv',\n",
       " 'real_53.csv',\n",
       " 'A3Benchmark-TS30.csv',\n",
       " 'A3Benchmark-TS24.csv',\n",
       " 'A3Benchmark-TS18.csv',\n",
       " 'A4Benchmark-TS87.csv',\n",
       " 'A4Benchmark-TS93.csv',\n",
       " 'A4Benchmark-TS44.csv',\n",
       " 'synthetic_70.csv',\n",
       " 'synthetic_64.csv',\n",
       " 'A4Benchmark-TS50.csv',\n",
       " 'A4Benchmark-TS78.csv',\n",
       " 'synthetic_58.csv',\n",
       " 'synthetic_59.csv',\n",
       " 'A4Benchmark-TS79.csv',\n",
       " 'synthetic_65.csv',\n",
       " 'A4Benchmark-TS51.csv',\n",
       " 'A4Benchmark-TS45.csv',\n",
       " 'synthetic_71.csv',\n",
       " 'A4Benchmark-TS92.csv',\n",
       " 'A4Benchmark-TS86.csv',\n",
       " 'A3Benchmark-TS19.csv',\n",
       " 'A3Benchmark-TS25.csv',\n",
       " 'A3Benchmark-TS31.csv',\n",
       " 'real_52.csv',\n",
       " 'real_46.csv',\n",
       " 'real_42.csv',\n",
       " 'real_56.csv',\n",
       " 'A3Benchmark-TS35.csv',\n",
       " 'A3Benchmark-TS21.csv',\n",
       " 'A4Benchmark-TS82.csv',\n",
       " 'A4Benchmark-TS96.csv',\n",
       " 'synthetic_49.csv',\n",
       " 'A4Benchmark-TS69.csv',\n",
       " 'synthetic_75.csv',\n",
       " 'A4Benchmark-TS41.csv',\n",
       " 'A4Benchmark-TS55.csv',\n",
       " 'synthetic_61.csv',\n",
       " 'A4Benchmark-TS54.csv',\n",
       " 'synthetic_60.csv',\n",
       " 'synthetic_74.csv',\n",
       " 'A4Benchmark-TS40.csv',\n",
       " 'A4Benchmark-TS68.csv',\n",
       " 'synthetic_48.csv',\n",
       " 'A4Benchmark-TS97.csv',\n",
       " 'A4Benchmark-TS83.csv',\n",
       " 'A3Benchmark-TS20.csv',\n",
       " 'A3Benchmark-TS34.csv',\n",
       " 'real_57.csv',\n",
       " 'real_43.csv',\n",
       " 'real_55.csv',\n",
       " 'real_41.csv',\n",
       " 'A3Benchmark-TS22.csv',\n",
       " 'A3Benchmark-TS36.csv',\n",
       " 'synthetic_89.csv',\n",
       " 'A4Benchmark-TS95.csv',\n",
       " 'A4Benchmark-TS81.csv',\n",
       " 'synthetic_62.csv',\n",
       " 'A4Benchmark-TS56.csv',\n",
       " 'A4Benchmark-TS42.csv',\n",
       " 'synthetic_76.csv',\n",
       " 'A4Benchmark-TS43.csv',\n",
       " 'synthetic_77.csv',\n",
       " 'synthetic_63.csv',\n",
       " 'A4Benchmark-TS57.csv',\n",
       " 'A4Benchmark-TS80.csv',\n",
       " 'A4Benchmark-TS94.csv',\n",
       " 'synthetic_88.csv',\n",
       " 'A3Benchmark-TS37.csv',\n",
       " 'A3Benchmark-TS23.csv',\n",
       " 'real_40.csv',\n",
       " 'real_54.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than period\n",
    "amp_window_size=4\n",
    "# (maybe) as same as period\n",
    "series_window_size=4\n",
    "# a number enough larger than period\n",
    "score_window_size=16\n",
    "\n",
    "spec = Silency(amp_window_size, series_window_size, score_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "\n",
    "    test_signal = df.value.values \n",
    "\n",
    "    score = spec.generate_anomaly_score_(test_signal)\n",
    "\n",
    "    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "\n",
    "    cutoff = np.percentile(score, 99)\n",
    "\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "#    pred_anom = np.zeros(len(test_signal))\n",
    "#    for i in range(int(len(test_signal))):\n",
    "#        for j in index_changes:\n",
    "#            pred_anom[j] = 1\n",
    "\n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40723272960410317, 0.8852972149525491, 0.5047092172414838)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than period\n",
    "amp_window_size=3\n",
    "# (maybe) as same as period\n",
    "series_window_size=3\n",
    "# a number enough larger than period\n",
    "score_window_size=32\n",
    "\n",
    "spec = Silency(amp_window_size, series_window_size, score_window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed threshold '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST parameter set, F1_score = 74.57%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/Users/manjunath.adinarayan/TIME-SERIES/YAHOO_Data/A4Benchmark/'\n",
    "path = '/Users/manjunath.adinarayan/TIME-SERIES/YAHOO_Data/Full_data/'\n",
    "\n",
    "extension='csv'\n",
    "os.chdir( path )\n",
    "files = glob.glob('*.{}'.format(extension))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 3.6278698444366455 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7787483979520454, 0.8016182114371371, 0.7457641660484401, 367)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = np.percentile(score, 99)\n",
    "    cutoff = 4\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "print(\"Time taken =\", (time.time()-st), \"seconds\")\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list), len(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Fscore of Yahoo's A1, A2, A3, A4\n",
    "#A1 = 32.23%\n",
    "#A2 = 73.008%\n",
    "#A3 = 91.19%\n",
    "#A4 = 87.89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HALF DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/Users/manjunath.adinarayan/TIME-SERIES/YAHOO_Data/A4Benchmark/'\n",
    "path = '/Users/manjunath.adinarayan/TIME-SERIES/YAHOO_Data/Full_data/'\n",
    "\n",
    "extension='csv'\n",
    "os.chdir( path )\n",
    "files = glob.glob('*.{}'.format(extension))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 3.999303102493286 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.784573328578778, 0.7552290545053432, 0.7320365590665248)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2):, :]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = np.percentile(score, 99)\n",
    "    cutoff = 4\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "print(\"Time taken =\", (time.time()-st), \"seconds\")\n",
    "        \n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Fscore of Yahoo's A1, A2, A3, A4\n",
    "#A1 = 27.78%\n",
    "#A2 = 78.6%\n",
    "#A3 = 89.25%\n",
    "#A4 = 82.14%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced SR with no averaging/filtering of saliency map\n",
    "# and\n",
    "# cutoff = 7 * np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_anomaly_score(self, values, type=\"avg\"):\n",
    "\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        \n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "        \n",
    "        return mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6509540021083907, 0.7779652423245614, 0.6508797391830556)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score2(test_signal)\n",
    "    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = np.percentile(score, 99)\n",
    "    cutoff = 7 * np.mean(score)\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.675088970218398, 0.7560466291248261, 0.6570270138255867)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2):, :]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score2(test_signal)\n",
    "    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = np.percentile(score, 99)\n",
    "    cutoff = 7 * np.mean(score)\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced SR with averaging/filtering of saliency map\n",
    "# cutoff = 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_anomaly_score(self, values, type=\"avg\"):\n",
    "#    extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "#    mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "#    ave_filter = Box_filter(mag, self.score_window_size)\n",
    "#    score = (mag - ave_filter) / ave_filter\n",
    "#    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5968055594659185, 0.8690790440124405, 0.6590783629749551)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = 2.8\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6589798720628981, 0.8312036686714798, 0.6940316418644611)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2):, :]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = 2.8\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half of the data [ TEST ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.784573328578778, 0.7552290545053432, 0.7320365590665248)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2): , : ]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = np.percentile(score, 99)\n",
    "    cutoff = 4\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.20945774908666842, 0.8896150391957784, 0.3147837013819169)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "\n",
    "    test_signal = df.value.values\n",
    "\n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "\n",
    "    index_changes = np.where(score > np.percentile(score, 98))[0]\n",
    "\n",
    "    cutoff = np.percentile(score, 98)\n",
    "\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "\n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for file in files:\n",
    "    if \"Benchmark\" in file:\n",
    "        cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
